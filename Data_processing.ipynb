{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Importing Libraries and Constants](#toc1_)    \n",
    "  - [Libraries](#toc1_1_)    \n",
    "  - [Constants](#toc1_2_)    \n",
    "- [Extracting Essential Tables For The Simulator](#toc2_)    \n",
    "  - [Read Dataset Files](#toc2_1_)    \n",
    "  - [Creating Training/Test set of VMs](#toc2_2_)    \n",
    "  - [Creating an Initial Probability Distribution for Each VM Configuration (Used by Audible Algorithm)](#toc2_3_)    \n",
    "  - [Creating a Conservative Initial VM Usage Model Based on Configuration (Used by Gaussian Algorithm)](#toc2_4_)    \n",
    "  - [Storing VMIDs with Their Sizes for Simulation](#toc2_5_)    \n",
    "  - [Create vmid to trace dataframe](#toc2_6_)    \n",
    "    - [Departure Rate](#toc2_6_1_)    \n",
    "    - [Create a vmid to trace file](#toc2_6_2_)    \n",
    "  - [Store tables of vmid to predicted average/variance for both Oracle and Estimation Approaches](#toc2_7_)    \n",
    "  - [Look-up table for Standard Normal](#toc2_8_)    \n",
    "  - [Reserved Values per VM for Resource Central](#toc2_9_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Importing Libraries and Constants](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Constants](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# BVM tuple: (baseline, num_cpus, ram, init_credits, credit_cap, price, name')\n",
    "bvm_configs = [\n",
    "    (3.37, 20,  80, 600, 4860, 0.992, 'Standard_B20ms'),        # B20MS\n",
    "    (2.70, 16,  64, 480, 3888, 0.794, 'Standard_B16ms'),        # B16MS\n",
    "    (2.02, 12,  48, 360, 2909, 0.595, 'Standard_B12ms'),        # B12MS\n",
    "    (1.35,  8,  32, 240, 1944, 0.397, 'Standard_B8ms'),        # B8MS\n",
    "    (0.90,  4,  16, 120, 1296, 0.198, 'Standard_B4ms'),        # B4MS\n",
    "    (0.60,  2,   8,  60,  864, 0.0992, 'Standard_B2ms'),        # B2MS\n",
    "    (0.40,  2,   4,  60,  576, 0.0496, 'Standard_B2s'),        # B2S\n",
    "    (0.20,  1,   2,  30,  288, 0.0248, 'Standard_B1ms'),        # B1MS\n",
    "    (0.10,  1,   1,  30,  144, 0.0124, 'Standard_B1s'),        # B1S\n",
    "    (0.05,  1, 0.5,  30,   72, 0.0062, 'Standard_B1ls')        # B1LS\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3.37: 20,\n",
       " 2.7: 16,\n",
       " 2.02: 12,\n",
       " 1.35: 8,\n",
       " 0.9: 4,\n",
       " 0.6: 2,\n",
       " 0.4: 2,\n",
       " 0.2: 1,\n",
       " 0.1: 1,\n",
       " 0.05: 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_to_corecount = {c[0]:c[1] for c in bvm_configs}\n",
    "baseline_to_corecount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Extracting Essential Tables For The Simulator](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Read Dataset Files](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_burstable dataframe has 721341 VM records\n"
     ]
    }
   ],
   "source": [
    "trace_dataframes, real_avg_cpu_utils_per_df = {}, {}\n",
    "for fn in ['2021_burstable']:#['2021_burstable', '2021_regular', '2019_burstable']:\n",
    "    # download the large \n",
    "    df = pd.read_parquet('data/{}VMs_{}_Data.parquet'.format(fn.split('_')[1].title(), fn.split('_')[0]))\n",
    "    # df.set_index('vmid', inplace = True)\n",
    "    trace_dataframes[fn] = df\n",
    "    real_avg_cpu_utils_per_df[fn] = df['trace'].to_dict()\n",
    "    print(f'{fn} dataframe has {df.shape[0]} VM records')\n",
    "    df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Creating Training/Test set of VMs](#toc0_)\n",
    "We divide the virtual machines into training and test sets. We use the training set to build the initial model and the test set for conducting simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_vmids_per_df, test_vmids_per_df = {}, {}\n",
    "for ds_name in trace_dataframes:\n",
    "    vmids = trace_dataframes[ds_name].index.values\n",
    "    # split the vmids to test and training\n",
    "    training_vmids_per_df[ds_name], test_vmids_per_df[ds_name] = train_test_split(trace_dataframes[ds_name].index.values, test_size=int( 0.2*len(trace_dataframes[ds_name]) ),random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Creating an Initial Probability Distribution for Each VM Configuration (Used by Audible Algorithm)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00,  7.04it/s]\n"
     ]
    }
   ],
   "source": [
    "for ds_name in trace_dataframes:\n",
    "    temp_training_df = trace_dataframes[ds_name].loc[training_vmids_per_df[ds_name]]\n",
    "\n",
    "    # for training, get the 95 percentile of the pdf for each vmcorecount\n",
    "    temp_training_df['avg_plus_std_first_hour'] = temp_training_df.index.map(lambda vmid: np.mean(real_avg_cpu_utils_per_df[ds_name][vmid][:12]) + np.std(real_avg_cpu_utils_per_df[ds_name][vmid][:12])) #['real_avg_avg_cpu'] + temp_training_df['real_std_avg_cpu']**(0.5)\n",
    "    # the cpu usage array for each vmi is in real_avg_cpu_utils_per_df['2021_regular'][vmid] and create pmf for each of them with accuracy of 1 decimal point from 0 to vmcorecount associated to vmid (inclusive)\n",
    "\n",
    "    # vmid_to_vmcorecount is a dictionary that include all the corecounts associated to a vmid\n",
    "    # take the top 95 percentile of the pmfs from the trianing vms\n",
    "    size_str = 'baseline' if 'burstable' in ds_name else 'corecount'\n",
    "    all_usage_per_corecount = {}\n",
    "    for corecount in tqdm(np.sort(temp_training_df[size_str].unique())):\n",
    "        temp = temp_training_df[(temp_training_df[size_str] == corecount)]['avg_plus_std_first_hour'].dropna()\n",
    "        # temp = vmid_to_f_per_df[name][(vmid_to_f_per_df[name]['corecount'] == core) & (vmid_to_f_per_df[name]['dram'] == ram)]['avg_plus_std_first_hour']\n",
    "        vmids = temp[temp >= temp.quantile(0.95)].index.values\n",
    "        all_usages = np.concatenate([real_avg_cpu_utils_per_df[ds_name][vmid][:12] for vmid in vmids])\n",
    "        all_usage_per_corecount[corecount] = all_usages\n",
    "\n",
    "    probability_density_per_corecount = {}\n",
    "    for corecount in np.sort(temp_training_df[size_str].unique()):\n",
    "        coef = corecount if 'regular' in ds_name else baseline_to_corecount[corecount]\n",
    "        y, x = np.histogram(all_usage_per_corecount[corecount], bins = np.array(range(int(coef)*100 + 2))/100, density = True)\n",
    "        probability_density_per_corecount[corecount] = y/100\n",
    "    # store the probability_density_training_vms\n",
    "    np.save(f'data/probability_density_training_vms_per_{size_str}_{ds_name}.npy', probability_density_per_corecount, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[Creating a Conservative Initial VM Usage Model Based on Configuration (Used by Gaussian Algorithm)](#toc0_)\n",
    "This model represents the 95th percentile of initial CPU usage averages and variances, grouped by each VM configuration within the training set VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_name in trace_dataframes:\n",
    "    feature_names = ['baseline' if 'burstable' in ds_name else 'corecount']# in a larger data set, this value could be set to ['corecount', 'dram']\n",
    "    try:\n",
    "        feature_to_avgs_training_arrivals = np.load( 'data/' + '{}_to_avgs_training_arrivals_{}.npy'.format('_'.join(feature_names), ds_name), allow_pickle=True)\n",
    "        feature_to_vars_training_arrivals = np.load( 'data/' + '{}_to_vars_training_arrivals_{}.npy'.format('_'.join(feature_names), ds_name), allow_pickle=True)\n",
    "    except:\n",
    "        temp_training_df = trace_dataframes[ds_name].loc[training_vmids_per_df[ds_name]]\n",
    "        feature_to_vmids = (temp_training_df[temp_training_df.vmcreated != 0].groupby(feature_names).apply(lambda x: x.index.tolist())).to_dict()\n",
    "        feature_to_avgs_training_arrivals, feature_to_vars_training_arrivals = {}, {}\n",
    "        for feature_value in tqdm(feature_to_vmids):\n",
    "            feature_to_avgs_training_arrivals[feature_value], feature_to_vars_training_arrivals[feature_value] = [], []\n",
    "            for vmid in tqdm(feature_to_vmids[feature_value]):\n",
    "                feature_to_avgs_training_arrivals[feature_value].append(np.mean(real_avg_cpu_utils_per_df[ds_name][vmid][:12]))\n",
    "                if len(real_avg_cpu_utils_per_df[ds_name][vmid]) > 5:\n",
    "                    feature_to_vars_training_arrivals[feature_value].append(np.var(real_avg_cpu_utils_per_df[ds_name][vmid][:12]))\n",
    "        np.save( 'data/' + '{}_to_avgs_training_arrivals_{}.npy'.format('_'.join(feature_names), ds_name), feature_to_avgs_training_arrivals)\n",
    "        np.save( 'data/' + '{}_to_vars_training_arrivals_{}.npy'.format('_'.join(feature_names), ds_name), feature_to_vars_training_arrivals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate first model quantiles 95\n",
    "def get_avg_var_per_core_ram(feature_names, ds_name):\n",
    "    p = 'data/'\n",
    "    feature_to_avgs = np.load(p + '{}_to_avgs_training_arrivals_{}.npy'.format('_'.join(feature_names), ds_name), allow_pickle = True).reshape(1, )[0]\n",
    "    feature_to_vars = np.load(p + '{}_to_vars_training_arrivals_{}.npy'.format('_'.join(feature_names),ds_name), allow_pickle = True).reshape(1, )[0]\n",
    "    avg_var_per_feature = {}\n",
    "    for feature_value in feature_to_avgs:\n",
    "            a = np.quantile(feature_to_avgs[feature_value], 0.95)\n",
    "            b = np.quantile(feature_to_vars[feature_value], 0.95)\n",
    "            avg_var_per_feature[feature_value] = (a, b)\n",
    "    return avg_var_per_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_var_per_feature_per_df = {}\n",
    "for ds_name in trace_dataframes:\n",
    "    feature_names = ['baseline' if 'burstable' in ds_name else 'corecount']\n",
    "    avg_var_per_feature_per_df[ds_name] = get_avg_var_per_core_ram(feature_names, ds_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_5_'></a>[Storing VMIDs with Their Sizes for Simulation](#toc0_)\n",
    "Store VMIDs from the test set that arrive during the monitoring window to ensure their initial usage is captured. In addition, store the baseline core performance for these VMs, which is the baseline value for burstable VMs and the peak core count for regular VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of test arrival vmids\n",
    "test_arrival_vmids_per_df, vmid_to_baseline_per_df = {}, {}\n",
    "for ds_name in trace_dataframes:\n",
    "    size_str = 'baseline' if 'burstable' in ds_name else 'corecount'\n",
    "    try:\n",
    "        test_arrival_vmids_per_df[ds_name] = np.load('data/test_arrival_vmids_' + ds_name + '.npy')\n",
    "        vmid_to_baseline_per_df[ds_name] = np.load( 'data/' + 'vmid_to_{}_{}.npy'.format(size_str, ds_name), allow_pickle=True).reshape(1, )[0]\n",
    "    except:\n",
    "        # in-progress VMs should not be used in simulation because their arrival usage in unknow and they have truncated data.\n",
    "        test_arrival_vmids_per_df[ds_name] = np.intersect1d(test_vmids_per_df[ds_name], trace_dataframes[ds_name][trace_dataframes[ds_name].vmcreated != 0].index.values)\n",
    "        np.save( 'data/test_arrival_vmids_' + ds_name + '.npy', test_arrival_vmids_per_df[ds_name])\n",
    "        # vmid to corecount\n",
    "        vmid_to_baseline_per_df[ds_name] = trace_dataframes[ds_name].loc[test_arrival_vmids_per_df[ds_name]][size_str].to_dict()\n",
    "        np.save( 'data/' + 'vmid_to_{}_{}.npy'.format(size_str, ds_name), vmid_to_baseline_per_df[ds_name])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_6_'></a>[Create vmid to trace dataframe](#toc0_)\n",
    "Our monitoring captures only a limited period of VM activities, which is likely to be shorter than the actual lifespan of some of the VMs. This situation leads to incomplete data. To address this, we analyzed VM departure rates to predict their lifespans beyond our observation and adjusted their operational periods in our simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_1_'></a>[Departure Rate](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure_rates_per_df = {}\n",
    "for ds_name in trace_dataframes:\n",
    "    temp = trace_dataframes[ds_name]\n",
    "    departure_rates = []\n",
    "    len_tamp1 = []\n",
    "    for i in range(2, 8):\n",
    "        # long-running Arrival VMS that at least i days of them could have been observed\n",
    "        if i == 7:\n",
    "            cond1 = (temp.vmcreated >= 0) # arrival VMs        \n",
    "            cond2 = (i*24*3600 <= (temp.vmdeleted.max() + 300 - temp.vmcreated))\n",
    "        else:\n",
    "            cond1 = (temp.vmcreated > 0) # arrival VMs\n",
    "            cond2 = (i*24*3600 <= (temp.vmdeleted.max() - temp.vmcreated)) # at least i days after creation exists in monitoring\n",
    "        cond3 = ( (i-1)*24*3600 < (temp.vmdeleted - temp.vmcreated)) # at least (i-1) day have been alive\n",
    "        temp1 = temp[cond1 & cond2 & cond3]\n",
    "        # VMs that lived i days\n",
    "        temp2 = temp1[(temp1.vmdeleted >= (temp1.vmcreated + (i-1)*24*3600) ) & (temp1.vmdeleted < (temp1.vmcreated + (i)*24*3600)) & ( temp1.vmdeleted != temp.vmdeleted.max()) ]\n",
    "        len_tamp1.append(len(temp1))\n",
    "        departure_rates.append(len(temp2)/len(temp1))\n",
    "    departure_rates_per_df[ds_name] = departure_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_6_2_'></a>[Create a vmid to trace file](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 503384/503384 [00:12<00:00, 39774.96it/s]\n"
     ]
    }
   ],
   "source": [
    "def determine_lr_num_samples(seen_num_samples, departure_rates):\n",
    "    \"\"\"\n",
    "    the seen_nu_samples are always greater or equal 288\n",
    "    \"\"\"\n",
    "    np.random.seed(seen_num_samples)\n",
    "    i = min( int(np.floor(seen_num_samples/288)), 6) - 1\n",
    "    alive = True\n",
    "    while alive:\n",
    "        alive = np.random.choice([True, False], p = [(1- departure_rates[min(5,i)]), departure_rates[min(5,i)]])\n",
    "        i += 1\n",
    "    new_num_samples = min(i * 288, 365*288) # to ensure at most one year worth of samples get added\n",
    "    return new_num_samples\n",
    "\n",
    "def gen_trace(trace, num_samples, revisions = [72, 288]):\n",
    "    \"\"\" \n",
    "    return a new trace which is real values of trace that has been concatnated with same array excluding the first interval in revisions to match num_samples  \n",
    "    precondition\n",
    "    len(trace)>revisions[0]\n",
    "    \"\"\"\n",
    "    first_revision = revisions[0]\n",
    "    # determine the new trace based on the given num_samples\n",
    "    if len(trace) < num_samples:\n",
    "        new_trace = np.zeros(num_samples)\n",
    "        num_rep = int(np.ceil(num_samples/len(trace[first_revision:])))\n",
    "        new_trace[:first_revision] = trace[:first_revision]\n",
    "        new_trace[first_revision:] = np.tile(trace[first_revision:], num_rep)[:len(new_trace)-first_revision]\n",
    "    else:\n",
    "        new_trace = np.copy(trace)\n",
    "    return new_trace\n",
    "\n",
    "new_traces_per_df = {}\n",
    "for ds_name in trace_dataframes:\n",
    "    new_traces_per_df[ds_name] = {}\n",
    "    vmids_unknown_end = trace_dataframes[ds_name][trace_dataframes[ds_name].vmdeleted == trace_dataframes[ds_name].vmdeleted.max()].index.values\n",
    "    for vmid in tqdm(test_arrival_vmids_per_df[ds_name]):\n",
    "        trace_len = len(real_avg_cpu_utils_per_df[ds_name][vmid])\n",
    "        if trace_len >= 288 and vmid in vmids_unknown_end: \n",
    "            new_traces_per_df[ds_name][vmid] = gen_trace(real_avg_cpu_utils_per_df[ds_name][vmid], determine_lr_num_samples(trace_len, departure_rates_per_df[ds_name]))\n",
    "        else:\n",
    "            new_traces_per_df[ds_name][vmid] = real_avg_cpu_utils_per_df[ds_name][vmid]\n",
    "\n",
    "    # save the datafile as feather\n",
    "    temp = pd.DataFrame(list(new_traces_per_df[ds_name].items()), columns = ['vmid', 'trace'])\n",
    "    temp.to_feather('data/' + 'test_arrival_vmid_to_trace_{}.feather'.format(ds_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_7_'></a>[Store tables of vmid to predicted average/variance for both Oracle and Estimation Approaches](#toc0_)\n",
    "Each VM's predicted average/variance are calculated by combining the initial conservative model with lifespan prediction. These predictions are generated both from an oracle perspective and by estimating according to the VM's early-stage CPU usage traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_avgs_vars(trace, vmsize, avg_or_var = 'avg' ,prediction = 'oracle'):\n",
    "    if prediction == 'oracle' and avg_or_var == 'avg':\n",
    "        return np.concatenate([np.ones(min(72, len(trace)))*avg_var_per_feature_per_df[ds_name][vmsize][0], np.mean(trace[72:])*np.ones(len(trace[72:]))])\n",
    "    if prediction == 'oracle' and avg_or_var == 'var':\n",
    "        return np.concatenate([np.ones(min(72, len(trace)))*avg_var_per_feature_per_df[ds_name][vmsize][1], np.var(trace[72:])*np.ones(len(trace[72:]))])\n",
    "    if prediction == 'est' and avg_or_var == 'avg':\n",
    "        return np.concatenate([np.ones(min(72, len(trace)))*avg_var_per_feature_per_df[ds_name][vmsize][0], np.mean(trace[:72])*np.ones(len(trace[72:]))])\n",
    "    if prediction == 'est' and avg_or_var == 'var':\n",
    "        return np.concatenate([np.ones(min(72, len(trace)))*avg_var_per_feature_per_df[ds_name][vmsize][1], np.var(trace[:72])*np.ones(len(trace[72:]))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3702: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py:221: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py:253: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for ds_name in trace_dataframes:\n",
    "    # read vmid to their associated traces\n",
    "    temp = pd.read_feather('data/' + 'test_arrival_vmid_to_trace_{}.feather'.format(ds_name))\n",
    "    # read vmid to their base performance\n",
    "    size_str = 'baseline' if 'burstable' in ds_name else 'corecount'\n",
    "    # Generate and save a table that has vmid to the predicted average/variance of the VM for each point in their lifetime \n",
    "    for prediction_type in ['oracle', 'est']: # Oracle and estimation prediction\n",
    "        temp['trace_pred_avgs'] = temp.apply(lambda row:  gen_avgs_vars(row['trace'], vmid_to_baseline_per_df[ds_name][row['vmid']], avg_or_var = 'avg' ,prediction = prediction_type), axis = 1)\n",
    "        temp['trace_pred_vars'] = temp.apply(lambda row:  gen_avgs_vars(row['trace'], vmid_to_baseline_per_df[ds_name][row['vmid']], avg_or_var = 'var' ,prediction = prediction_type), axis = 1)\n",
    "        temp[['vmid', 'trace_pred_avgs', 'trace_pred_vars']].to_feather('data/' + '0.95_first_model_{}_rest_{}.feather'.format(prediction_type, ds_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_8_'></a>[Look-up table for Standard Normal](#toc0_)\n",
    "For faster CDF computation within the Gaussian algorithm, we create data ahead of time offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1176/999999 [00:00<02:46, 6012.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999999/999999 [02:17<00:00, 7271.71it/s]\n"
     ]
    }
   ],
   "source": [
    "precision = 6\n",
    "lookup_table = {}\n",
    "for desired_cdf in tqdm(np.arange(0 + 1/(10**precision), 1 , 1/(10**precision))):\n",
    "    corresponding_value = norm.ppf(desired_cdf)\n",
    "    lookup_table[round(desired_cdf, precision)] = corresponding_value\n",
    "np.save('data/' + 'standard_lookup_table_precision_{}.npy'.format(precision), lookup_table, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_9_'></a>[Reserved Values per VM for Resource Central](#toc0_)\n",
    "- Resource Central uses bucket-based CPU usage forecasts.\n",
    "- More buckets are created for burstable VMs, which usually stay below baseline but sometimes exceed it.\n",
    "- Oracle predictions identify the bucket representing the 95th percentile CPU usage for each VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buckets_per_baseline(PEAK_CPU_PER_BASELINE):\n",
    "    # The buckets for predicting in the resource central method\n",
    "    BUCKET_RC_PER_BASELINE = {}\n",
    "    for baseline in PEAK_CPU_PER_BASELINE:\n",
    "        values1 = np.linspace(0, baseline, num = 5, endpoint=False)\n",
    "        values2 = np.linspace(baseline, PEAK_CPU_PER_BASELINE[baseline], num = 4, endpoint=True)\n",
    "        BUCKET_RC_PER_BASELINE[baseline] = np.concatenate((values1, values2), axis=0)[1:]\n",
    "    return BUCKET_RC_PER_BASELINE\n",
    "\n",
    "def get_allocated_cores(BUCKET_RC_PER_BASELINE, vmid_to_baseline, vmid_to_trace, prediction_quantile):\n",
    "    # for all the traces, calculate the bucket value that bound their usage\n",
    "    bucketized_predictions_per_vmid = {}\n",
    "    for vmid in vmid_to_trace:\n",
    "        baseline, trace = vmid_to_baseline[vmid], vmid_to_trace[vmid]\n",
    "        idx = np.argmin(BUCKET_RC_PER_BASELINE[baseline] <= np.quantile(trace, prediction_quantile))\n",
    "        bucketized_predictions_per_vmid[vmid] = BUCKET_RC_PER_BASELINE[baseline][idx]\n",
    "    return bucketized_predictions_per_vmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_quantile = 0.95 # In resource central, we picked a bucket that bounds the 95 percentile of the CPU usage\n",
    "for ds_name in trace_dataframes:\n",
    "    size_str = 'baseline' if 'burstable' in ds_name else 'corecount'\n",
    "    vmid_to_trace = pd.read_feather('data/' + 'test_arrival_vmid_to_trace_{}.feather'.format(ds_name)).set_index('vmid')['trace'].to_dict()\n",
    "    try:\n",
    "        vmid_to_baseline = vmid_to_baseline_per_df[ds_name]\n",
    "    except:\n",
    "        vmid_to_baseline = np.load( 'data/' + 'vmid_to_{}_{}.npy'.format(size_str, ds_name), allow_pickle=True).reshape(1, )[0]\n",
    "    if 'burstable' in ds_name:\n",
    "        PEAK_CPU_PER_BASELINE = {3.37: 20, 2.7: 16, 2.02: 12, 1.35: 8, 0.9: 4, 0.6: 2, 0.4: 2, 0.2: 1, 0.1: 1, 0.05: 1}\n",
    "    else:\n",
    "        unique_baselines = np.array(list(set(vmid_to_baseline.values())))\n",
    "        PEAK_CPU_PER_BASELINE = dict(zip(unique_baselines, unique_baselines))# we extend resource central to improve accuracy since there is more chance the VM's 95 percentile fall in the first 20% of Peak CPU\n",
    "\n",
    "\n",
    "    BUCKET_RC_PER_BASELINE = get_buckets_per_baseline(PEAK_CPU_PER_BASELINE)\n",
    "    allocated_cores_per_vmid = get_allocated_cores(BUCKET_RC_PER_BASELINE, vmid_to_baseline, vmid_to_trace, prediction_quantile)\n",
    "    \n",
    "    temp = pd.DataFrame(data = list(allocated_cores_per_vmid.values()), index = list(allocated_cores_per_vmid.keys()), columns = [f'first_rc_{prediction_quantile}'])\n",
    "    temp.index.name = 'vmid'\n",
    "    temp.reset_index().to_feather('data/' + f\"first_models_quantile_{prediction_quantile}_{ds_name}.feather\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
